{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Yfr_Vlr8HBkt",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Zomata Data Analysis Project\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name**            - Tanya Garg"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today’s digital era, food discovery platforms like Zomato play a vital role in connecting customers with restaurants. One of the most important factors influencing customer decision-making is the approximate cost of dining at a restaurant. Accurate cost estimation helps users plan their visits better and assists restaurant owners in positioning their offerings competitively. This project focuses on building a machine learning-based predictive system that estimates the approximate cost for two people at a restaurant using metadata available from Zomato.\n",
        "\n",
        "The dataset used in this project consists of restaurant-level information such as restaurant name, cuisines offered, restaurant collections or categories, and the average cost for two people. Since the target variable, cost, is numeric in nature, the problem is framed as a supervised regression problem. The objective is to learn the relationship between restaurant attributes and their pricing structure and use this relationship to predict costs for unseen data.\n",
        "\n",
        "A significant portion of the project is dedicated to data preprocessing and cleaning, which is essential for creating a production-ready machine learning pipeline. The cost column contained commas and non-numeric values, which were cleaned and converted into numerical format. Irrelevant features such as restaurant links and timings were removed to reduce noise. Missing values were handled carefully to ensure the model was trained on high-quality data. Exception handling techniques were applied to make the code robust and error-free when executed in one go, satisfying deployment-ready requirements.\n",
        "\n",
        "Since important features such as cuisines and collections are textual, feature engineering played a crucial role in this project. Text data was transformed into numerical form using TF-IDF (Term Frequency–Inverse Document Frequency) vectorization, which captures the importance of words while reducing the impact of commonly occurring terms. This approach allows machine learning models to interpret textual information effectively and learn meaningful patterns related to restaurant pricing.\n",
        "\n",
        "To gain deeper insights into the dataset, extensive exploratory data analysis (EDA) was performed following the UBM (Univariate, Bivariate, and Multivariate) analysis approach. More than fifteen meaningful visualizations were created to analyze cost distribution, popular cuisines, cost variation across restaurant categories, and relationships between multiple features. Each visualization was accompanied by business-focused interpretations explaining how the insights could impact pricing strategies, customer targeting, and revenue optimization. Both positive and negative growth indicators were identified and justified based on observed trends.\n",
        "\n",
        "Multiple machine learning algorithms were implemented and compared, including Linear Regression, Ridge Regression, and Random Forest Regressor. Model performance was evaluated using metrics such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R² score, each of which was interpreted in a business context. Cross-validation and hyperparameter tuning were applied to improve model performance, and improvements were documented using evaluation score comparison charts.\n",
        "\n",
        "In conclusion, this project demonstrates how machine learning can be effectively applied to predict restaurant costs using real-world data. The final solution is well-structured, fully executable, and production-ready. Beyond technical implementation, the project emphasizes business impact, interpretability, and decision-making value, making it a strong example of practical machine learning applied in the food technology domain."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To design a supervised machine learning model that estimates restaurant pricing using Zomato metadata.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data handling and numerical operations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning tools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Model evaluation metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(\"Zomato Restaurant names and Metadata.csv\")\n",
        "\n",
        "    print(\"Dataset loaded successfully ✅\")\n",
        "    print(\"Dataset Shape (Rows, Columns):\", df.shape)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Dataset file not found. Please check the file name or path.\")\n",
        "except Exception as e:\n",
        "    print(\"An unexpected error occurred while loading the dataset:\", e)\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df.isnull(), cbar=False)\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After exploring the dataset, I understood that it contains information about restaurants listed on Zomato, including restaurant names, cuisines, collections, and the approximate cost for two people. The dataset has both textual features (such as cuisines and collections) and a numerical feature (cost), which makes it suitable for a machine learning regression problem.\n",
        "\n",
        "By viewing the dataset structure using basic functions, I learned the number of rows and columns, the names of different features, and the data types of each column. I also checked for duplicate values and missing values, which helped me understand the quality of the data. I found that the dataset required basic cleaning, especially in the cost column, before it could be used for modeling.\n",
        "\n",
        "Overall, this exploration helped me clearly identify the target variable (Cost) and the input features, and prepared the dataset for further preprocessing, visualization, and machine learning model development.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Represents the name of the restaurant listed on Zomato.\n",
        "\n",
        "Cuisines: Indicates the types of cuisines served by the restaurant.\n",
        "\n",
        "Collections: Describes the category or collection to which the restaurant belongs.\n",
        "\n",
        "Cost: Represents the approximate cost for two people at the restaurant and serves as the target variable for prediction.\n",
        "\n",
        "Links: Contains the Zomato webpage link of the restaurant.\n",
        "\n",
        "Timings: Shows the operating hours of the restaurant."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop_duplicates()\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "df['Cost'] = df['Cost'].str.replace(',', '')\n",
        "df['Cost'] = pd.to_numeric(df['Cost'], errors='coerce')\n",
        "\n",
        "df = df.drop(columns=['Links', 'Timings'])\n",
        "\n",
        "df.shape"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duplicate records were removed and missing values were handled to improve data quality. The cost column was cleaned and converted into numerical format, and irrelevant columns were dropped. These manipulations revealed that restaurant pricing varies with cuisines and collections, and the cleaned dataset became structured, consistent, and suitable for reliable analysis and machine learning modeling."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "\n",
        "sns.histplot(\n",
        "    df['Cost'],\n",
        "    bins=20,\n",
        "\n",
        ")\n",
        "\n",
        "plt.title(\"Distribution of Restaurant Cost\")\n",
        "plt.xlabel(\"Cost for Two People\")\n",
        "plt.ylabel(\"Number of Restaurants\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I picked this histogram because it is the best chart to understand the distribution of a numerical variable like restaurant cost. It helps visualize how costs are spread across restaurants and identify common price ranges, skewness, and extreme values in the dataset.**"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The chart shows that most restaurants fall within a lower to mid-price range, while only a few restaurants have very high costs. This indicates that budget and mid-range restaurants are more common on the platform compared to premium or high-priced restaurants.**"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes, the insights can create a positive business impact by helping Zomato focus on promoting budget and mid-range restaurants, which attract a larger customer base. However, the presence of fewer high-cost restaurants may indicate limited demand in the premium segment, which could lead to slower growth if not targeted toward niche or high-income customers.**"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "# Split multiple cuisines and explode\n",
        "cuisine_series = df['Cuisines'].str.split(',').explode().str.strip()\n",
        "\n",
        "# Plot top 10 cuisines\n",
        "cuisine_series.value_counts().head(10).sort_values().plot(kind='barh')\n",
        "\n",
        "plt.title(\"Top 10 Most Common Cuisines\")\n",
        "plt.xlabel(\"Number of Restaurants\")\n",
        "plt.ylabel(\"Cuisine Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I chose a bar chart because it is suitable for comparing frequencies of categorical variables. It clearly shows which cuisines are most commonly offered by restaurants on the platform.**"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The chart shows that a few cuisines dominate the market, with some cuisine types appearing much more frequently than others. This indicates popular food preferences among customers.**"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes, the insights help in focusing promotions and recommendations on popular cuisines to attract more users. However, over-reliance on common cuisines may limit diversity, potentially reducing customer interest in the long term.**"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "# Explode the 'Cuisines' column to have one cuisine per row, while keeping the 'Cost'\n",
        "df_exploded = df.assign(Cuisines=df['Cuisines'].str.split(',')).explode('Cuisines')\n",
        "df_exploded['Cuisines'] = df_exploded['Cuisines'].str.strip()\n",
        "\n",
        "# Get the top 5 most common cuisines from the exploded dataframe\n",
        "top_5_cuisines = df_exploded['Cuisines'].value_counts().head(5).index\n",
        "\n",
        "sns.boxplot(\n",
        "    y='Cuisines',\n",
        "    x='Cost',\n",
        "    data=df_exploded[df_exploded['Cuisines'].isin(top_5_cuisines)]\n",
        ")\n",
        "\n",
        "plt.title(\"Cost Distribution Across Top 5 Cuisines\")\n",
        "plt.xlabel(\"Cost for Two People\")\n",
        "plt.ylabel(\"Cuisine Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I chose a box plot because it effectively compares the distribution of a numerical variable (cost) across different categories (cuisines). It helps identify median cost, variation, and outliers for each cuisine type.**\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The chart shows that different cuisines have different cost ranges. Some cuisines have higher median costs and wider variations, indicating premium pricing, while others are more budget-friendly with lower and consistent costs.**"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes, these insights help in pricing strategy and targeted promotions by matching cuisines with customer budgets. However, cuisines with very high cost variability may discourage price-sensitive customers, potentially leading to lower demand in those segments.**"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "df['Collections'].value_counts().head(10).plot(kind='bar')\n",
        "plt.title(\"Top 10 Restaurant Collections\")\n",
        "plt.xlabel(\"Collection Type\")\n",
        "plt.ylabel(\"Number of Restaurants\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A histogram is suitable to understand the distribution of a numerical variable like cost.**"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Most restaurants fall in the low to mid-price range, while very few are high-priced.**"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes, it helps focus on budget and mid-range customers. Fewer premium restaurants indicate limited demand in that segment.**"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.boxplot(y=df['Cost'])\n",
        "plt.title(\"Overall Cost Distribution\")\n",
        "plt.ylabel(\"Cost for Two People\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Boxplots summarize cost spread, median, and outliers.**"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are significant outliers, showing a few very expensive restaurant.**"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps identify premium pricing segments. Extreme prices may discourage average users.**"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_cuisines = df['Cuisines'].value_counts().head(10).index\n",
        "avg_cost = df[df['Cuisines'].isin(top_cuisines)].groupby('Cuisines')['Cost'].mean()\n",
        "\n",
        "avg_cost.plot(kind='bar', figsize=(8,4))\n",
        "plt.title(\"Average Cost by Cuisine\")\n",
        "plt.ylabel(\"Average Cost\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bar charts clearly compare average costs across cuisines.**"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Some cuisines have consistently higher average costs.**"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Useful for pricing and recommendation systems. Expensive cuisines may target niche customers.**"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_collections = df['Collections'].value_counts().head(10).index\n",
        "avg_cost_col = df[df['Collections'].isin(top_collections)].groupby('Collections')['Cost'].mean()\n",
        "\n",
        "avg_cost_col.plot(kind='bar', figsize=(8,4))\n",
        "plt.title(\"Average Cost by Collection\")\n",
        "plt.ylabel(\"Average Cost\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It helps compare pricing across restaurant categories.**"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Premium collections have higher average costs than casual ones.**"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supports collection-based marketing strategies. High costs may limit mass appeal.**"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Cuisine_Count'] = df['Cuisines'].apply(lambda x: len(x.split(',')))\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.scatterplot(x='Cuisine_Count', y='Cost', data=df)\n",
        "plt.title(\"Cuisine Count vs Cost\")\n",
        "plt.xlabel(\"Number of Cuisines\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A scatter plot shows the relationship between two numerical variables.**"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restaurants offering more cuisines generally have higher costs.**"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps restaurants decide menu size. Too many cuisines may increase operational cost.**"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "sns.boxplot(x='Collections', y='Cost', data=df[df['Collections'].isin(top_collections)])\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Cost Distribution by Collection\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Boxplots effectively compare cost variation across collections.**"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Some collections show wide cost variation, others are consistent.**"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps identify stable vs volatile pricing categories. High variation may confuse customers.**"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.kdeplot(df['Cost'], fill=True)\n",
        "plt.title(\"Cost Density Distribution\")\n",
        "plt.xlabel(\"Cost\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Density plots help understand overall cost concentration smoothly.**"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Costs are concentrated in a specific range with a right-skewed distribution.**"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps identify dominant price bands. Extreme prices may contribute less to revenue.**"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "sns.boxplot(x='Collections', y='Cost', hue='Cuisines',\n",
        "            data=df[df['Cuisines'].isin(top_cuisines) & df['Collections'].isin(top_collections)])\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multivariate boxplots help analyze combined effects of multiple variables.**"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost depends on both cuisine type and restaurant collection.**"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supports personalized recommendations. Complex combinations may need targeted marketing.**"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pivot = pd.pivot_table(df, values='Cost', index='Cuisines',\n",
        "                       columns='Collections', aggfunc='mean')\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(pivot, cmap='coolwarm')\n",
        "plt.title(\"Average Cost Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Heatmaps clearly show patterns across two categorical variables.**"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Certain cuisine-collection combinations are consistently expensive.**"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps identify premium combinations. Less popular combinations may need promotions.**"
      ],
      "metadata": {
        "id": "FkPmSeABTMlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df[['Cost', 'Cuisine_Count']])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pairplots show relationships and distributions together.**"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Higher cuisine counts often relate to higher costs.**"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helps optimize menu design. Excessive variety may increase costs without proportional demand.**"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(df[['Cost', 'Cuisine_Count']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A correlation heatmap is chosen because it clearly shows the strength and direction of relationships between numerical variables using color intensity and values.**"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The heatmap shows a positive correlation between the number of cuisines offered and the restaurant cost, indicating that restaurants offering more cuisines tend to have higher costs.**"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.pairplot(df[['Cost', 'Cuisine_Count']])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A pair plot is chosen because it helps visualize relationships, trends, and distributions between multiple numerical variables simultaneously in a simple and clear manner.**"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The plot shows how restaurant cost changes with the number of cuisines offered. It also highlights the distribution of each variable and indicates that higher cuisine counts are generally associated with higher costs.**"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restaurants offering multiple cuisines have a higher average cost than restaurants offering a single cuisine**.\n",
        "\n",
        "**Hypotheses**:\n",
        "\n",
        "**Null Hypothesis (H₀): There is no significant difference in cost between single-cuisine and multi-cuisine restaurants.**\n",
        "\n",
        "**Alternative Hypothesis (H₁): Multi-cuisine restaurants have a significantly higher cost.**"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Create cuisine count\n",
        "df['Cuisine_Count'] = df['Cuisines'].apply(lambda x: len(x.split(',')))\n",
        "\n",
        "# Split data\n",
        "single_cuisine = df[df['Cuisine_Count'] == 1]['Cost']\n",
        "multi_cuisine = df[df['Cuisine_Count'] > 1]['Cost']\n",
        "\n",
        "# Perform t-test\n",
        "t_stat, p_value = ttest_ind(single_cuisine, multi_cuisine, equal_var=False)\n",
        "\n",
        "t_stat, p_value\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Independent Samples t-test**"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This test was chosen because:**\n",
        "\n",
        "**1-The comparison is between two independent groups**\n",
        "\n",
        "**2-The target variable (Cost) is numerical**\n",
        "\n",
        "**3-The goal is to compare mean values**\n",
        "\n",
        "**The independent t-test is the most appropriate test in this scenario.**\n",
        "\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Average restaurant cost differs significantly across different restaurant collections.**\n",
        "\n",
        "**Hypotheses:**\n",
        "\n",
        "**Null Hypothesis (H₀): Mean cost is the same across all collections.**\n",
        "\n",
        "**Alternative Hypothesis (H₁): Mean cost differs for at least one collection.**"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "top_collections = df['Collections'].value_counts().head(5).index\n",
        "\n",
        "groups = [df[df['Collections'] == col]['Cost'] for col in top_collections]\n",
        "\n",
        "f_stat, p_value = f_oneway(*groups)\n",
        "\n",
        "f_stat, p_value\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A One-Way ANOVA (Analysis of Variance) test was used.**"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANOVA was chosen because:**\n",
        "\n",
        "**1-There are more than two categories (collections)**\n",
        "\n",
        "**2-The dependent variable (Cost) is numerical**\n",
        "\n",
        "**3-The objective is to compare mean values across multiple groups.**\n",
        "\n",
        "Using multiple t-tests would increase error, so ANOVA is statistically appropriate.**"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There is a significant positive relationship between the number of cuisines offered and restaurant cost.**\n",
        "\n",
        "**Hypotheses:**\n",
        "\n",
        "**Null Hypothesis (H₀): There is no correlation between cuisine count and cost.**\n",
        "\n",
        "**Alternative Hypothesis (H₁): There is a significant positive correlation between cuisine count and cost.**"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "corr_coeff, p_value = pearsonr(df['Cuisine_Count'], df['Cost'])\n",
        "\n",
        "corr_coeff, p_value\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Pearson Correlation Test was used.**"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This test was chosen because:**\n",
        "\n",
        "**1-Both variables are continuous numerical variables.**\n",
        "\n",
        "**2-The goal is to measure relationship, not difference.**\n",
        "\n",
        "**3-Pearson correlation is standard for checking linear associatio.**"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.isnull().sum()\n",
        "df['Cost'].fillna(df['Cost'].median(), inplace=True)\n",
        "df['Cuisines'].fillna(df['Cuisines'].mode()[0], inplace=True)\n",
        "df['Collections'].fillna(df['Collections'].mode()[0], inplace=True)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Median imputation was applied to the numerical feature Cost to reduce the influence of outliers, while mode imputation was used for categorical features to preserve the most frequent category distribution. These methods are computationally efficient, minimize bias, and are well-suited for structured tabular data.**"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = df['Cost'].quantile(0.25)\n",
        "Q3 = df['Cost'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "\n",
        "df['Cost'] = np.where(df['Cost'] < lower_bound, lower_bound,\n",
        "                       np.where(df['Cost'] > upper_bound, upper_bound, df['Cost']))\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Interquartile Range (IQR) method was used for outlier treatment by identifying values beyond 1.5×IQR. Outliers were handled using capping to retain extreme observations within acceptable limits. This approach minimizes the influence of extreme values while preserving data size and maintaining a realistic cost distribution.**"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize label encoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Encode categorical columns\n",
        "df['Cuisines_encoded'] = le.fit_transform(df['Cuisines'])\n",
        "df['Collections_encoded'] = le.fit_transform(df['Collections'])\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Encoding was applied to transform categorical features (Cuisines and Collections) into numerical representations. It was chosen due to its computational efficiency, low memory overhead, and direct compatibility with regression algorithms, enabling seamless model training without increasing feature dimensionality.**"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand contractions manually (safe & lightweight)\n",
        "\n",
        "contraction_map = {\n",
        "    \"can't\": \"cannot\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"n't\": \" not\",\n",
        "    \"'re\": \" are\",\n",
        "    \"'s\": \" is\",\n",
        "    \"'d\": \" would\",\n",
        "    \"'ll\": \" will\",\n",
        "    \"'t\": \" not\",\n",
        "    \"'ve\": \" have\",\n",
        "    \"'m\": \" am\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text):\n",
        "    for key, value in contraction_map.items():\n",
        "        text = text.replace(key, value)\n",
        "    return text\n",
        "\n",
        "df['Cuisines'] = df['Cuisines'].astype(str).apply(expand_contractions)\n",
        "df['Collections'] = df['Collections'].astype(str).apply(expand_contractions)\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Cuisines'] = df['Cuisines'].str.lower()\n",
        "df['Collections'] = df['Collections'].str.lower()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "df['Cuisines'] = df['Cuisines'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
        "df['Collections'] = df['Collections'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Remove URLs\n",
        "df['Cuisines'] = df['Cuisines'].apply(lambda x: re.sub(r'http\\S+|www\\S+', '', x))\n",
        "df['Collections'] = df['Collections'].apply(lambda x: re.sub(r'http\\S+|www\\S+', '', x))\n",
        "\n",
        "# Remove words containing digits and standalone digits\n",
        "df['Cuisines'] = df['Cuisines'].apply(lambda x: re.sub(r'\\b\\w*\\d\\w*\\b', '', x))\n",
        "df['Collections'] = df['Collections'].apply(lambda x: re.sub(r'\\b\\w*\\d\\w*\\b', '', x))\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "df['Cuisines'] = df['Cuisines'].apply(lambda x: ' '.join([w for w in x.split() if w not in stop_words]))\n",
        "df['Collections'] = df['Collections'].apply(lambda x: ' '.join([w for w in x.split() if w not in stop_words]))\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove extra white spaces\n",
        "df['Cuisines'] = df['Cuisines'].str.strip()\n",
        "df['Collections'] = df['Collections'].str.strip()\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Cuisines'] = df['Cuisines'].astype(str)\n",
        "df['Collections'] = df['Collections'].astype(str)\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Cuisines_tokens'] = df['Cuisines'].apply(lambda x: x.split())\n",
        "df['Collections_tokens'] = df['Collections'].apply(lambda x: x.split())\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Normalization using Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "df['Cuisines'] = df['Cuisines'].apply(lambda x: ' '.join(ps.stem(w) for w in x.split()))\n",
        "df['Collections'] = df['Collections'].apply(lambda x: ' '.join(ps.stem(w) for w in x.split()))\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming was used as the text normalization technique. It reduces words to their root form, helping decrease vocabulary size and improve model efficiency. Stemming is computationally simple and suitable for basic text preprocessing in regression-based machine learning projects.**"
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "df['Cuisines_pos'] = df['Cuisines'].apply(lambda x: nltk.pos_tag(x.split()))\n",
        "df['Collections_pos'] = df['Collections'].apply(lambda x: nltk.pos_tag(x.split()))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text using TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=300)\n",
        "\n",
        "X_cuisines = tfidf.fit_transform(df['Cuisines'])\n",
        "X_collections = tfidf.fit_transform(df['Collections'])\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF (Term Frequency–Inverse Document Frequency) was used for text vectorization because it converts textual data into meaningful numerical features by capturing word importance while reducing the impact of frequently occurring, less informative terms. It is efficient, interpretable, and well-suited for machine learning models.**"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new combined text feature\n",
        "df['combined_text'] = df['Cuisines'] + ' ' + df['Collections']\n",
        "\n",
        "# Create numerical feature: cuisine count\n",
        "df['Cuisine_Count'] = df['Cuisines'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Drop highly correlated / redundant features\n",
        "df = df.drop(columns=['Cuisines_encoded', 'Collections_encoded'], errors='ignore')\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select relevant features to avoid overfitting\n",
        "\n",
        "# Final feature set\n",
        "X = df[['Cuisine_Count']]  # numerical feature\n",
        "y = df['Cost']\n",
        "\n",
        "# TF-IDF features from combined text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=200)\n",
        "X_text = tfidf.fit_transform(df['combined_text'])\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I used manual feature selection based on domain understanding and exploratory data analysis. Only relevant features such as Cuisine_Count and TF-IDF text features were retained, while redundant or highly correlated variables were removed. This approach helps reduce model complexity, minimize overfitting, and improve model generalization and interpretability.**"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The most important features identified were Cuisine_Count, Cuisines, and Collections.\n",
        "Cuisine_Count is important because restaurants offering more cuisines generally have higher operational and pricing levels.\n",
        "Cuisines and Collections are important because they capture the type and category of restaurants, which strongly influence pricing patterns.**"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes, the data required transformation because the numerical feature (Cuisine_Count) had different scale compared to the target variable. Standard Scaling was applied to normalize the feature by centering it around zero with unit variance. This helps improve model stability, ensures fair contribution of features, and enhances performance for regression-based algorithms**"
      ],
      "metadata": {
        "id": "MGJf4YAJe3Yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform numerical features using Standard Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standard Scaling was used to scale the data because it standardizes features to have zero mean and unit variance. This prevents features with larger magnitudes from dominating the model and improves the performance and convergence of regression algorithms.**"
      ],
      "metadata": {
        "id": "buyofAeffK0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensionality reduction is not strictly required in this project because the number of numerical features is limited and TF-IDF features were already constrained using a maximum feature limit. The feature space is manageable, helps preserve interpretability, and reduces the risk of losing important information. However, dimensionality reduction techniques could be considered if the feature space becomes very large or if model performance needs further optimization.**"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction using PCA (Optional)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "X_reduced = pca.fit_transform(X_text.toarray())\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Principal Component Analysis (PCA) was used for dimensionality reduction because it transforms high-dimensional TF-IDF features into a smaller set of uncorrelated components while retaining maximum variance. This helps reduce computational complexity, minimize noise, and improve model efficiency without significantly losing important information**"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets (80:20 ratio)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**An 80:20 train–test split was used, where 80% of the data was allocated for training and 20% for testing. This ratio provides sufficient data for the model to learn patterns while reserving enough unseen data to reliably evaluate model performance and generalization**"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No, the dataset is not considered imbalanced because this is a regression problem, not a classification problem. Imbalance is mainly a concern when class labels are unevenly distributed. Here, the target variable (Cost) is continuous, and its values are reasonably spread across a range, making imbalance handling unnecessary.**"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 : Linear Regression\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize the model\n",
        "lr_model = LinearRegression()\n",
        "\n",
        "# Fit the model on training data\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_lr = lr_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression was used as a baseline regression model to predict restaurant cost by learning a linear relationship between input features and the target variable. After hyperparameter tuning using Ridge Regression, the model showed improved performance with lower MAE and RMSE and a higher R² score in the evaluation metric score chart, indicating better generalization and reduced overfitting.**"
      ],
      "metadata": {
        "id": "hjNbNlGqjlo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Calculate evaluation metrics for Linear Regression\n",
        "mae = mean_absolute_error(y_test, y_pred_lr)\n",
        "mse = mean_squared_error(y_test, y_pred_lr)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "mae, rmse, r2\n",
        "\n"
      ],
      "metadata": {
        "id": "dXnjfm7lUTNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = ['MAE', 'RMSE', 'R2 Score']\n",
        "values = [mae, rmse, r2]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(metrics, values)\n",
        "plt.title(\"Evaluation Metric Scores - Linear Regression\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 : Linear Regression with Hyperparameter Optimization (GridSearchCV)\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'alpha': [0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "# GridSearch CV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=ridge,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error'\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_ridge = best_model.predict(X_test)\n",
        "\n",
        "grid_search.best_params_\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I used GridSearchCV for hyperparameter optimization. It was chosen because it systematically evaluates all possible combinations of predefined hyperparameters using cross-validation. This ensures the selection of optimal parameters, improves model generalization, and is well-suited for models with a small and well-defined parameter space like Ridge Regression.**"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes, after applying GridSearchCV with Ridge Regression, the model performance improved compared to the baseline Linear Regression model. Regularization helped reduce overfitting and improved the model’s generalization on unseen data.**"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluation metrics for Ridge Regression\n",
        "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "rmse_ridge = np.sqrt(mse_ridge)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "mae_ridge, rmse_ridge, r2_ridge\n",
        "\n"
      ],
      "metadata": {
        "id": "2RihQ_hI-v_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = ['MAE', 'RMSE', 'R2 Score']\n",
        "\n",
        "before_tuning = [mae, rmse, r2]              # Linear Regression\n",
        "after_tuning = [mae_ridge, rmse_ridge, r2_ridge]  # Ridge Regression\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar(x - width/2, before_tuning, width, label='Before Tuning (Linear)')\n",
        "plt.bar(x + width/2, after_tuning, width, label='After Tuning (Ridge)')\n",
        "\n",
        "plt.xticks(x, metrics)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Model-1 Performance: Before vs After Tuning\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "0SQI4Lxq-0Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 : Random Forest Regressor\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the model\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_rf = rf_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "hm1d8sW3ivwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest Regressor is an ensemble-based model that combines multiple decision trees to capture complex, non-linear relationships in the data. The evaluation metric score chart shows that it achieved lower prediction errors (MAE and RMSE) and a higher R² score compared to Model 1, demonstrating superior predictive performance and robustness.**"
      ],
      "metadata": {
        "id": "1KBgXwJtjX4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluation metrics for Random Forest Regressor\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mse_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "mae_rf, rmse_rf, r2_rf\n"
      ],
      "metadata": {
        "id": "usQR6ZU0Vinn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = ['MAE', 'RMSE', 'R2 Score']\n",
        "values_rf = [mae_rf, rmse_rf, r2_rf]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(metrics, values_rf)\n",
        "plt.title(\"Evaluation Metric Scores - Random Forest Regressor\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 : Random Forest Regressor with Hyperparameter Optimization (GridSearchCV)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Initialize base model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search_rf = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid_rf,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Get best model\n",
        "best_rf_model = grid_search_rf.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_rf_opt = best_rf_model.predict(X_test)\n",
        "\n",
        "# Display best parameters\n",
        "grid_search_rf.best_params_\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GridSearchCV was used for hyperparameter optimization because it systematically evaluates all combinations of selected hyperparameters using cross-validation. This ensures the selection of optimal parameter values, improves model generalization, and is effective when the hyperparameter search space is well-defined, as in Random Forest Regression.**"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after applying GridSearchCV to Random Forest Regressor, the model performance improved. The optimized model achieved lower MAE and RMSE and a higher R² score compared to the non-tuned version, indicating better prediction accuracy and generalization."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluation metrics for Optimized Random Forest\n",
        "mae_rf_opt = mean_absolute_error(y_test, y_pred_rf_opt)\n",
        "mse_rf_opt = mean_squared_error(y_test, y_pred_rf_opt)\n",
        "rmse_rf_opt = np.sqrt(mse_rf_opt)\n",
        "r2_rf_opt = r2_score(y_test, y_pred_rf_opt)\n",
        "\n",
        "mae_rf_opt, rmse_rf_opt, r2_rf_opt\n",
        "\n"
      ],
      "metadata": {
        "id": "JHHQN4Ir_BgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = ['MAE', 'RMSE', 'R2 Score']\n",
        "\n",
        "before_tuning = [mae_rf, rmse_rf, r2_rf]\n",
        "after_tuning = [mae_rf_opt, rmse_rf_opt, r2_rf_opt]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar(x - width/2, before_tuning, width, label='Before Tuning')\n",
        "plt.bar(x + width/2, after_tuning, width, label='After Tuning')\n",
        "\n",
        "plt.xticks(x, metrics)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Model-2 Performance: Random Forest Before vs After Tuning\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "CRz8qF6R-GqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Mean Absolute Error (MAE)**\n",
        "\n",
        "**MAE represents the average absolute difference between the predicted restaurant cost and the actual cost.\n",
        "Business Impact:\n",
        "A lower MAE means the model’s predictions are close to real prices, leading to more reliable cost estimates for users. This improves customer trust and helps restaurants position themselves accurately on the platform.**\n",
        "\n",
        "**2. Root Mean Squared Error (RMSE)**\n",
        "\n",
        "**RMSE measures the prediction error while penalizing larger errors more heavily.\n",
        "Business Impact:\n",
        "A lower RMSE ensures that the model avoids large pricing mistakes, which is important because big cost prediction errors can mislead customers and negatively impact user experience and restaurant credibility.**\n",
        "\n",
        "**3. R² Score (Coefficient of Determination)**\n",
        "\n",
        "**R² indicates how much of the variation in restaurant cost is explained by the model.\n",
        "Business Impact:\n",
        "A higher R² score shows that the model captures key pricing factors effectively. This helps businesses make data-driven pricing, recommendation, and market segmentation decisions with greater confidence.**\n",
        "\n",
        "**Overall Business Impact of the ML Model**\n",
        "\n",
        "**By achieving lower MAE and RMSE along with a higher R² score, the ML model provides accurate and consistent cost predictions. This supports better customer decision-making, improves restaurant visibility strategies, and enhances the overall reliability of the food discovery platform.**"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 : Gradient Boosting Regressor\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Initialize the model\n",
        "gbr_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Fit the model on training data\n",
        "gbr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_gbr = gbr_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Evaluation metrics for Gradient Boosting Regressor\n",
        "mae_gbr = mean_absolute_error(y_test, y_pred_gbr)\n",
        "mse_gbr = mean_squared_error(y_test, y_pred_gbr)\n",
        "rmse_gbr = np.sqrt(mse_gbr)\n",
        "r2_gbr = r2_score(y_test, y_pred_gbr)\n",
        "\n",
        "mae_gbr, rmse_gbr, r2_gbr\n"
      ],
      "metadata": {
        "id": "Qtx1owQKW4af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Evaluation Metric Score Chart – Model 3 (Gradient Boosting Regressor)\n",
        "metrics = ['MAE', 'RMSE', 'R2 Score']\n",
        "values_gbr = [mae_gbr, rmse_gbr, r2_gbr]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(metrics, values_gbr)\n",
        "plt.title(\"Evaluation Metric Scores - Gradient Boosting Regressor\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 : Gradient Boosting Regressor with Hyperparameter Optimization (GridSearchCV)\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Initialize base model\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid_gbr = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search_gbr = GridSearchCV(\n",
        "    estimator=gbr,\n",
        "    param_grid=param_grid_gbr,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error'\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search_gbr.fit(X_train, y_train)\n",
        "\n",
        "# Get best model\n",
        "best_gbr_model = grid_search_gbr.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_gbr_opt = best_gbr_model.predict(X_test)\n",
        "\n",
        "# Best hyperparameters\n",
        "grid_search_gbr.best_params_\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GridSearchCV was used for hyperparameter optimization because it exhaustively searches all combinations of specified hyperparameters using cross-validation. This ensures optimal parameter selection, improves model generalization, and is suitable when the hyperparameter space is limited and well-defined, as in Gradient Boosting Regression**"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After applying GridSearchCV, the Gradient Boosting model showed improved performance. The optimized model produced lower MAE and RMSE and a higher R² score compared to the baseline Gradient Boosting model, indicating better accuracy and generalization on unseen data.**"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics for Optimized Gradient Boosting Regressor\n",
        "mae_gbr_opt = mean_absolute_error(y_test, y_pred_gbr_opt)\n",
        "mse_gbr_opt = mean_squared_error(y_test, y_pred_gbr_opt)\n",
        "rmse_gbr_opt = np.sqrt(mse_gbr_opt)\n",
        "r2_gbr_opt = r2_score(y_test, y_pred_gbr_opt)\n",
        "\n",
        "mae_gbr_opt, rmse_gbr_opt, r2_gbr_opt\n"
      ],
      "metadata": {
        "id": "yCjZDHe6XC82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = ['MAE', 'RMSE', 'R2 Score']\n",
        "\n",
        "before_tuning = [mae_gbr, rmse_gbr, r2_gbr]\n",
        "after_tuning = [mae_gbr_opt, rmse_gbr_opt, r2_gbr_opt]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar(x - width/2, before_tuning, width, label='Before Tuning')\n",
        "plt.bar(x + width/2, after_tuning, width, label='After Tuning')\n",
        "\n",
        "plt.xticks(x, metrics)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Gradient Boosting Performance Improvement After Tuning\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "btMms3P2XD5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For positive business impact, I primarily considered MAE, RMSE, and R² Score.**\n",
        "\n",
        "**MAE was considered because it directly represents the average pricing error, which helps ensure customers see accurate and trustworthy cost estimates.**\n",
        "\n",
        "**RMSE was important because it penalizes large prediction errors that could significantly mislead users and harm platform credibility.**\n",
        "\n",
        "**R² Score was used to understand how well the model explains pricing variability, helping businesses rely on the model for strategic pricing and recommendation decisions.**\n",
        "\n",
        "**Together, these metrics ensure both accuracy and reliability, leading to better customer experience and informed business decisions.**"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Among all the models tested (Linear/Ridge Regression, Random Forest Regressor, and Gradient Boosting Regressor), the optimized Gradient Boosting model achieved the lowest MAE and RMSE and the highest R² score. This indicates more accurate predictions, fewer large pricing errors, and better explanation of cost variability. Additionally, Gradient Boosting effectively captures complex, non-linear relationships in the data, making it the most reliable model for final restaurant cost prediction and positive business impact.**"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting Regressor is an ensemble learning algorithm that builds multiple weak decision trees sequentially. Each new tree corrects the errors of the previous ones, allowing the model to learn complex and non-linear relationships between features and restaurant cost. Due to its strong predictive performance and ability to handle mixed feature interactions, it was selected as the final prediction model.**"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best performing model (Gradient Boosting Regressor) for deployment\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(best_gbr_model, \"best_restaurant_cost_model.joblib\")\n",
        "\n",
        "print(\"Model saved successfully as best_restaurant_cost_model.joblib\")\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model and perform prediction on unseen data (Sanity Check)\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load(\"best_restaurant_cost_model.joblib\")\n",
        "\n",
        "# Create sample unseen input (example: Cuisine_Count = 3)\n",
        "unseen_data = np.array([[3]])\n",
        "\n",
        "# Predict using the loaded model\n",
        "predicted_cost = loaded_model.predict(unseen_data)\n",
        "\n",
        "predicted_cost\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this project, a complete end-to-end machine learning pipeline was developed to predict restaurant pricing using Zomato metadata. The dataset was thoroughly explored, cleaned, and preprocessed to ensure data quality and consistency. Extensive exploratory data analysis (EDA) was performed using univariate, bivariate, and multivariate visualizations to uncover meaningful patterns and business insights related to restaurant cost, cuisines, and collections.**\n",
        "\n",
        "**Multiple regression models were implemented and evaluated, including Linear/Ridge Regression, Random Forest Regressor, and Gradient Boosting Regressor. Model performance was assessed using MAE, RMSE, and R² score to ensure both statistical accuracy and positive business impact. Hyperparameter optimization using GridSearchCV further improved model performance and generalization. Among all models, the optimized Gradient Boosting Regressor delivered the best results with lower prediction errors and higher explanatory power.**\n",
        "\n",
        "**Feature engineering and explainability techniques helped identify key drivers of restaurant pricing, particularly the influence of cuisine variety and restaurant categories. The final model was saved and reloaded successfully, demonstrating deployment readiness. Overall, this project highlights how machine learning can provide accurate, interpretable, and business-relevant insights for restaurant cost prediction, supporting better decision-making for both customers and food service platforms.**"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}